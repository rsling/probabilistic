<<setupthrsmethds, cache=FALSE, echo=FALSE, include=FALSE, results='asis'>>=
  opts_knit$set(self.contained=FALSE)
@

\chapter{Theories, methods, and data}
\label{sec:theoriesmethodsanddata}

\section{Prototypes and exemplars}
\label{sec:prototypesandexemplars}

In this section I discuss one major overarching theoretical theme of the case studies, namely prototype theory and its rival, exemplar theory.
The central question is which type of cognitive representation research on alternations provides evidence for.%
\footnote{This is an extended and modified version of Section~1 from \citet{Schaefer2018} with additions from \citet{Schaefer2016c}.}
The typical approach in alternation research is to annotate a large number of corpus sentences with linguistic features and to model the probability of the variants being chosen given these features.
The idea is that a variant is chosen when the influencing features cumulatively assume typical values for that variant.
For several reasons, a variant of prototype theory with features \citep{Rosch1978} is a good candidate for the appropriate cognitive model.
I will first introduce prototype theory and exemplar theory, including a discussion of the state of the art in linguistics and cognitive science.
The discussion then focusses specifically on the data-driven corpus-based approaches which have been used very prominently in probabilistic modelling and alternation research (see \citealt{Gries2017a}), and whether such approaches have anything to contribute to the prototype vs.\ exemplar debate.

In Aristotelian approaches to linguistic categorisation, category membership is determined by rules and defining features, and it is consequently not viewed as a matter of degree (\citealt{Sutcliffe1993}; \citealt[11--16]{Murphy2002}).
However, based on evidence pointing to the fact that humans often categorise objects by similarity and with varying degrees of fuzziness, prototype theory (\citealt{Rosch1973}; see \citealt{Taylor2008} for an overview) and exemplar theory \citep{MedinSchaffer1978,Hintzman1986} were developed.
Prototype theory assumes that categories are defined by the similarity of their members to a mentally stored abstraction.
This abstraction takes the form of the most prototypical member or -- in later versions of the theory -- weighted features defining the prototype (see \RAWeakN\ for a detailed description of prototype theory with features including \textit{cue validity}).
Many researchers such as \citet{Gries2003,Gilquin2006,NessetJanda2010,Dobric2015} have used prototype theory in some form of corpus-based linguistic modelling.%
\footnote{In the influential framework of cognitive grammar \citep{Langacker1987}, prototypes (which, as should be remembered, represent abstractions already) are literally taken as prototypical exemplars, and there is an additional level of fully discrete abstractions in the form of schemas.
Schemas are characterised by the properties common to all members of the category, whereas a prototypical category member might have very specific additional properties not at all shared by all or even most members \citep[371-375]{Langacker1987}.
The prototype can serve as a reference point when classifying new objects which do not share all properties of the schema, but this would (if repeated) lead to the creation of an even more abstract (hierarchically higher and less specific) schema which describes the new member and the ones belonging to the previous schema.
As pointed out by \citet[136--137]{Langacker1987}, schemas and prototypes thus fulfil different roles and can be assumed to co-exist.
A strict exemplar view of language is incompatible, as far as I can see, with Langacker's view of schemas, but any theory of categorisation that allows for at least some kind of abstraction is not in fundamental contradiction with it.
In my research, I do not use schemas in my descriptions of the relevant categories, mostly because the aspect of similarity and fuzzy classification is central to probabilistic modelling, and a formulation in terms of schemas would bring about an unnecessarily high degree of abstraction (see \citealt[70--71]{Taylor2003} for a parallel argument).}

While prototype theory is well suited for modeling constructional choices, it has a prominent adversary in exemplar theory (\citealt{MedinSchaffer1978,Hintzman1986}).
Prototype theory and exemplar theory model essentially the same types of effects but differ significantly in whether they assume higher-level abstractions in the form of single maximally prototypical exemplars or their features (prototype theory) or assume that categories emerge through the storage of many exemplars and similarity classification on those exemplars (exemplar theory).
\citet{Barsalou1990} already showed that prototype and exemplar theory model the same types of surface effects and are informationally equivalent, at least when it comes to the results of cognitive agents' behaviour.
\citet[84]{Barsalou1990} states that

\begin{quote}
  we can not say whether category knowledge is distributed in exemplars or centralized abstractions.
  But we do know that any account of knowledge that excludes idiosyncratic information, cooccurrence information, or dynamic representation is inadequate.
\end{quote}

Consequently, research producing evidence in favour of one theory or the other commonly does not use mere output data but tests the procedural behaviour of subjects in controlled experiments, for example the speed of category retrieval.
In very early experiments, \citet{PosnerKeele1968} showed, for example, that highly prototypical unseen exemplars were categorised more easily by subjects compared to less prototypical ones which had been included in the data made available to them in order to learn the categories.
This was (at least at the time) taken as evidence that subjects categorise by prototypes.
Since corpus data only show artefacts of production events and we have no experimental access to the speaker's or writer's performance and their actual similarity judgements, one should be sceptical whether corpus analysis alone could ever decide which theory of mental representation is more suitable.
\citet[22]{Gries2003} can be taken as recognising this, when he says:

\begin{quote}
  Frequently, Rosch’s results were [\ldots] interpreted as if they were statements on the structure of mental representations as such; cf.\ the effects = structure fallacy and the prototype = representation fallacy.
  I do not wish to support such interpretations.
  [\ldots]
  Still, even if the form of analysis does not translate into statements on mental representations, the high predictive power [\ldots] shows that the cognitive factors underlying the choice of construction have been identified properly and weighted in accordance with their importance for actual usage.
\end{quote}

A similar caveat (without direct reference to prototypes and exemplars) can be found in \citet[486--487]{Dabrowska2016}, who states that we cannot ``deduce mental representations from patterns of use'', \ie\ from corpus data.\label{abs:dabrowska}
As corpus data are artefacts of cognitive agents' behaviour, they cannot decide between two theories for which Barsalou's informational equivalence criterion holds.

Given this situation, the question arises of how the discussion in the usage-based linguistics community connects with the current discussion in cognitive science.
In cognitive science, it is mostly accepted that exemplar theories have greater explanatory power \citep[184]{Vanpaemel2016} and that abstraction is only needed marginally, if at all.%
\footnote{The hard empirical evidence in favour of exemplar models is substantial.
For example, in \citet{HahnEa2010}, the authors show that subjects even use exemplar similarity over abstract knowledge even when they are given very simple explicit rules to be learned.
This is highly relevant because most other studies focus on the learning of implicit rule-based knowledge, which involves many auxiliary assumptions in actual experiments \citep[2]{HahnEa2010}.
On the other hand, there is evidence that neither theory is fully adequate to model humans' capabilities to form categories.
For example, \citet{ConawayKurtz2016} show that both prototype theory and exemplar theory fail to explain certain experimental results where subjects learn to generalise beyond the input in a way that cannot be explained by similarity.
}
Still, various attempts have been made over the past decades to settle the dispute between abstraction-based models (models with rules or prototypes) and exemplar models, or to find models which unite the two extremes.
\citet{VanpaemelStorms2008} and \citet{LeeVanpaemel2008} proposed the \textit{varying abstraction model} (VAM) which ``attempt[s] to balance economy and informativeness'' \citep[745]{LeeVanpaemel2008}, treating models with full abstraction (radical prototype theory) and no abstraction at all (radical exemplar theory) as special cases of a model which allows for both abstraction and exemplar effects.
The mixture model of categorisation (MMC) by \citet{Rosseel2002} is a model with abstraction in the form of hierarchical clusters of exemplars, and these clusters of objects are characterised by a probability distribution over their features, and categorising new objects is a process of estimating the probability of this object belonging to one of the clusters.
\citet{GriffithsEa2009} go further and present a computational model which is able to choose the appropriate complexity of representation for a given category.
However, despite these (and more) attempts to reconcile or unite the two approaches while developing spelled-out mathematical models, \citet[183--184]{Vanpaemel2016} describes the state of affairs between adherents of neo-prototype theory (such as \citealt{MindaSmith2001,MindaSmith2002}) and exemplar theory as a stalemate.

In cognitive linguistics, \citet{DivjakArppe2013} is a very rare example of a paper where such issues are taken up with reference to the current research in cognitive science.
Their corpus-based approach shows ``one way of systematically analyzing usage data as contained in corpora to yield a scheme, compatible with usage-based theories of language, by which the assumptions of both the prototype and exemplar theories can be operationalized'' \citep[267]{DivjakArppe2013}.
Their approach to implementing a varying abstraction model \citep[254--260]{DivjakArppe2013} is based on hierarchical clustering of annotated properties of sentences.
They cluster sentences containing Russian verbs of trying.
Then, they single out the one sentence from each cluster which scores the highest probability for any of the six \textit{try} verbs according to a polytomous regression model estimated on the same data.
The clusters are interpreted as intermediate-level exemplar-derived abstractions of typical contexts for these high-probability verbs (typically more than one cluster for each verb; \citealt[255--256]{DivjakArppe2013}).
The crucial difference between such data-driven corpus-based analyses and experiments in cognitive science (\citealt{DivjakArppe2013} use \citealt{VerbeemenEa2007} as their reference) is that cognitive research is based on experiments where subjects produce actual category assignments or similarity judgements, and in corpus studies, the categories and category membership are determined purely from existing data.
The experimental approach with reduced and\slash or artificial stimuli makes it much easier to examine very specific effects in the behaviour of the subjects.
While I do not think the results of the case study presented in \citet{DivjakArppe2013} are invalid, any data set can be analysed to yield a certain number of clusters, and this fact alone does not substantiate any claim about one mental representation or another.
Thus, the study does not ensure that the clusters emerging from the data correspond to any speaker's cognitive representation.
In other words, \citet[229--230]{DivjakArppe2013} fall victim to Barsalou's equivalence trap when they state without further motivation that

\begin{quote}
  [t]he objectives of this study are, first, to explore how the prototype and exemplar models of categorization manifest themselves in corpus data [\ldots].
  Although corpus data do not reflect the characteristics of mental grammars directly, we do consider corpus data a legitimate source of data about mental grammars.
\end{quote}

The second sentence of this quote has at least one reading in which it is contradictory.
Compare the (already mentioned) more realistic views in \citet[84]{Barsalou1990}, \citet[486--487]{Dabrowska2016} and \citet[22]{Gries2003}.

As mentioned above, in cognitive science, experimental setups which allow access to the cognitive agents' performance over time are preferred in order to produce evidence for either one of the two competing theories.
See \citet{StormsEa2000} for a comparison of the theories in different experimental settings.
However, the trade-off one has to accept when doing experiments with highly simplified stimuli and very simple tasks is their lower \textit{external validity} (\ie\ their lower degree of generalisability) and their high dependence on potentially problematic operationalisations of constructs, control of confounding factors in the face of a limited number of available subjects, etc. (in other words, critical dependence on \textit{construct validity} and \textit{internal validity}).%
\footnote{Construct validity requires the measurements made in a experiment to be credible and substantive indicators of a theoretically postulated construct (such as a prototype).
Under internal validity an experiment establishes a causal relationship between experimental manipulations and the measured effects through minimisation of systematic measurement error.
An accessible overview of the different types of validity can be found in Chapter~1 of \citet{MaxwellDelaney2004}.
The discussion of types of validity goes back to \citet{CronbachMeehl1955,CampbellFiske1959}.}
Tasks in cognitive science have been criticised exactly for their lack of external validity, for example by \citet{Murphy2003}.
From a linguistic perspective, it is remarkable in this context that \citet{VoorspoelsEa2011} consider their experimental task -- which is the assignment of typicality scores to nouns from the domains of \textit{animals} and \textit{artefacts} to categories like \textit{bird}, \textit{fish}, \textit{clothing}, or \textit{tools} -- a study of ``superordinate natural language categories, whereas most evidence supporting exemplar representations has been found in artificial categories of a more subordinate level'' \citep[1013]{VoorspoelsEa2011}.
Corpus linguists interested in probabilistic alternation modelling deal with much more complex high-level categories and use large and complex feature sets, especially in (morpho-)syntax.%
\footnote{Notice, however, that recently, approaches have emerged which solve at least some problems by abandoning linguistic high-level features altogether \citep{BaayenEa2016,RamscarPort2016}.
Clearly, they have not (or at least not yet) reached mainstream popularity, and it remains to be seen how well they perform on a broader range of questions.}
It is thus an advantage of much linguistic work on categorisation that it deals with complex and realistically produced data, because this greatly improves the external validity of studies, albeit by sacrificing some construct validity.
An ideal contribution by cognitive corpus linguists to the research on (levels of) category abstraction in the human mind would thus be to provide analyses which have great external validity and complexity while carefully making sure that (and determining to what extent) these finding correlate with reactions from cognitive agents under more controlled experimental conditions, which increases the construct validity.
This is why experimental validations of corpus-derived models should under all circumstances become the standard procedure.
Section~\ref{sec:corporaincognitivelyorientedlinguistics} briefly discusses this approach.

In closing, I want to point out that my work often conveniently uses prototype-theoretical formulations (\RAWeil, \RAWeakN, \RAMeasure) because the high-level contentful features which are mostly used in probabilistic modelling (such as semantic classes of lemmas, definiteness of noun phrases, discourse status, or register, to name just a few) invite a description that allows for abstractions.
However, \ROMeasure\ argues that certain types of effects are at least implausible to model as abstractions.
In the study, it is shown that lemma frequency and construction-lemma attraction influence the choice of alternants.
Such item-specific effects indeed appear to favour an exemplar view.
However, it must be noted that it is always possible that item-specific effects can in fact be traced back to abstraction effects (such as semantic properties of lemmas).
Again, in the spirit of Section~\ref{sec:probabilisticgrammar}, we should be aware that our inferences are more often than not abductive, \ie\ inferences to the (in the view of our research community) best explanation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Corpora in cognitively oriented linguistics}
\label{sec:corporaincognitivelyorientedlinguistics}

\subsection{Problems with corpora and some solutions}
\label{sec:problemswithcorporaandsomesolutions}

The empirical analysis of probabilistic phenomena such as alternations requires researchers to make choices with regard to the data they use for their scientific inferences.
Depending on the amount of data available and type of inference, methods for the numerical analysis of the data are also required.
In the present section, I argue why web corpora (\ie\ corpora built using material collected from the WWW) are ideal for the corpus-based work presented here, and I briefly introduce the idea of experimental validation of corpus findings.
Section~\ref{sec:statistics} will then discuss methods of statistical analysis.

Corpora have been used as a major source of data in alternation research and cognitively oriented linguistics in general, and my research is no exception.
Since cognitively oriented linguistics is an attempt to model cognitive representations as well as the cognitive mechanisms involved in using these representations to produce and understand utterances, the question arises whether corpus data -- \ie\ artefacts of language use -- are an appropriate source of data in cognitively oriented linguistics.

Prominently, \citet[591--592]{Gries2017b} argues that corpus linguistics is essentially the quantitative analyses of co-occurrence frequencies (\eg\ of words and words or words and constructions, words and senses) in collections of texts, which is often related to the \textit{distributional hypothesis} and traced back to \citet{Harris1954}.
Gries also notes that the major tenet of cognitively oriented linguistics is that language users learn language by acquiring knowledge about the probabilities of words, constructions, senses, etc. in a given context (in the broadest sense of the word \textit{context}).
Thus, Gries concludes, both disciplines deal with distributional phenomena and are highly compatible.
Clearly, this is accurate inasmuch as both corpus linguistics and cognitively oriented linguistics examine types of distributional phenomena, but one distributional phenomenon is not necessarily like any other one.
The implicit claim made by Gries is that both fields deal with \textit{the same or at least two highly and causally related distributional phenomena}.
While it is impossible to refute this implicit additional assumption, it is also difficult to substantiate it.
Therefore, I suggest that we accept it as a working hypothesis.
At least, however, the approach begs the question of whether corpora represent the cognitive reality of language users in any meaningful and reliable way.%
\footnote{I do, however, contradict the categorically contemptuous tone found in many works from cognitively oriented linguistics (\citealt[590--593]{Gries2017b} is no exception) against earlier work based on researchers' intuitions.
As \citet{SprouseAlmeida2012,SprouseEa2013} have shown with significant methodological rigour, judgements collected in a traditional way through intuition by linguists can be highly reliable.
The question is rather which types of data are used as evidence to support which claims.
Intuitive judgements (even by linguists) are not intrinsically unreliable; they might just be the wrong tool in certain situations.}
The traditional discussion of the \textit{representativeness} of a corpus does not necessarily help in this context, because it is more often than not centred around the concept of a corpus being \textit{representative of a language} as a whole, using as points of reference:
(i) the distribution of texts or text types in the output of all speakers of a language (production-based),
(ii) the distribution of the relevance of texts or text types in the whole speech community (relevance-based), or
(iii) the distribution of speakers' exposure to different texts or text types (perception-based).%
\footnote{For overviews from different perspectives, see \citet{Biber1993}, \citet{MceneryEa2006}, \citet{Leech2007}, \citet{Hunston2008}.
A summary of the discussion is found in Chapter~5 of \citet{SchaeferBildhauer2013}.}

Indeed, given the argumentation from \citet{Gries2017b} discussed above, a perception-based view of corpora seems to be the most appropriate for a cognitive approach to language where input frequencies play the most crucial role.
However, the linguistic experience of language users is most definitely a highly individual matter, and most corpora force researchers to work with highly problematic abstractions.
In a recent contribution where the percep\-tion-based view is argued to be valid in cognitively oriented corpus linguistics, \citet[104]{StefanowitschFlach2016} take a quite cavalier stance on representativeness:%
\footnote{The picture does not change significantly if corpora are seen as collections of linguistic output events under a cognitively oriented perspective \citep{TummersEa2005}.}

\begin{quote}
In this wider context, large, register-mixed corpora such as the British National Corpus [\ldots] may not be perfect models of the linguistic experience of adult speakers, but they are reasonably close to the input of an idealized average member of the relevant speech community.
\end{quote}

The assumption that an \textit{idealised average speaker} is a valid construct seems naïve at best.%
\footnote{Essentially, they argue for a license to conclude that any distributional pattern found in the BNC automatically has a cognitive reality.
If life were this easy, many more researchers would certainly use the BNC exclusively.}
If the concept of an idealised average speaker were admissible and if there were indeed corpora (like the BNC) representative of this idealised speaker's input, then different actual speakers should not learn considerably diverging grammars.
If it turns out that actual speakers indeed acquire fundamentally different grammars, however, then the idealisation is unwarranted.
There is growing evidence from both psycholinguistic research and cognitively oriented linguistics that differences between competent adult speakers of a language are substantial and should not be averaged.
This concerns speakers' performance in linguistic tasks (\citealt{HuettigJanse2016} and references therein), but it also affects their individual grammars.
For example, \citet{Dabrowska2008,Dabrowska2012} clearly found that there is no convergence of the grammars of different Polish adult speakers towards a unified grammar (with respect to the phenomena under study), and \citet{Dabrowska2015} puts this into a larger picture.
This does, of course, not entail that speakers would perceive each other's languages as \textit{different} like foreign languages or dialects -- or that they would be expected to have problems communicating in their everyday life.
However, this clearly questions the usefulness of the concept of an \textit{idealised average speaker}.
A commonly adopted solution is to model speaker-variation as a nuisance variable, usually by adding a per-speaker random effect to the statistical models (\citealt{Gries2015}; \citealt{Schaefer2018}; see Section~\ref{sec:statistics}).
Most corpora, however, lack metadata to identify the authors of specific texts reliably.
If they do, single authors usually contribute far too few data to take individual variation into account in an informative way.
But even if there are enough data, the random-effect approach is just a way of taking care of unmodelled heterogeneity (see also Section~\ref{sec:modeleverything}).
It does not make the remaining \textit{averaged} part of the model more cognitively real.%
\footnote{Although without specific empirical backup, \citet[695--698]{Newmeyer2003} already made this point convincingly, which is something one can admit without committing to the full argument he put forward.}

In the corpus linguistic discussion on representativeness (as mentioned above), this problem has received little attention, mostly because this discussion has traditionally focussed on a global notion of representativeness.
An ideal corpus of a language is assumed to be representative of a language (such as English or German) as a whole (whatever that means).
This holistic approach to the question of representativeness (which surely still inspired the view argued for by \citealt{StefanowitschFlach2016}) is not applicable to cognitively oriented linguistics, which probably requires techniques for experiments and observational studies similar to those in the social sciences, psychology, and cognitive science.
The goal in scientific experiments (or scientific studies, to use a more general term) is to make inferences about a \textit{population of interest} using a sample of data from that population.
The population of interest has to be defined with regard to each experiment individually, and it might be something very specific (such as the written output of speakers of a certain age, in a specific register, etc.) instead of \textit{the language} or \textit{the average speaker} (across all communicative settings and modes).
In the social sciences, the concepts of \textit{global and specific representativeness} \citep[86]{Bortz2005} are used to describe the relevant distinction.
The crucial point is thus not whether a corpus is \textit{representative of a language} but whether a sample taken for a specific purpose represents the population of interest for the concrete study.

A standard approach implicitly adopted in many corpus studies (including mine) is to define the population of interest as \textit{corpus exemplars in which a certain range of constructions, words, etc.\ occurs}.%
\footnote{This implicitly assumes something similar to what \citet[27]{BerkFreedman2009} call an \textit{imaginary population}.
Their criticism applies only in limited ways to corpus sampling given the argument I make here.}
This is effectively what we do if we run unrestricted queries looking for specific morphological or syntactic patterns in some large corpus like the DeReKo of the Institut für Deutsche Sprache (IDS; \citealt{KupietzEa2010}) or the DECOW16A \citep{SchaeferBildhauer2012,BiemannEa2013,SchaeferBildhauer2013,Schaefer2015b}.
With certain caveats taken into consideration and given the right research question, nothing speaks against this approach, as I will argue below.
Going beyond this unrestricted query approach, however, \citet{Gries2015,Gries2017b} argues that by using metadata (as available for the British National Corpus; \citealt{Lee2001,Burnard2007}), searches can be refined to specifically examine phenomena with different (relative) frequencies in different modes, genres, etc.
If there is no hypothesis that mode, register, etc. have an influence on (relative) frequencies, however, such refinements are not strictly required.%
\footnote{Note that the word \textit{influence} implies a \textit{causal relationship}.}

The unrestricted query approach works when the hypothesis is that the relative frequencies of two structures A and B change when going from condition x to condition y.
Mathematically, this hypothesis can be expressed as (\ref{eq:001}) (see also Section~\ref{sec:nevereverrandomfallacy}).%
\footnote{f(A|x) is to be read as in the notation for conditional probabilities, except with frequencies instead of probabilities.
  It is thus the \textit{frequency of item A under condition x}.}

\begin{equation}
  \frac{f(A|x)}{f(B|x)}\not =\frac{f(A|y)}{f(B|y)}
  \label{eq:001}
\end{equation}

For example, in \ROWeakN\ one hypothesis was that the frequency of the singular non-nominative form of a weak noun such as (\textit{den}\slash\textit{dem}\slash\textit{des}) \textit{Planeten} `planet' (structure A) compared to the frequency of its (non-standard) strong form (\textit{den}\slash\textit{dem}\slash\textit{des}) \textit{Planet} (structure B) is different in the genitive (condition x) than in the accusative and dative (condition y).

As long as a corpus contains both structures and \textit{given the randomness assumption} (RA), the hypothesis can be examined using an unrestricted query.
The RA holds if all other potential influencing factors which favour the occurrence of A or B are distributed equally in the conditions x and y.
In the example, individual lemmas might have a tendency to favour the strong form over the weak form or vice versa, or different registers might favour one form over the other etc.
However, given a random assignment of lemmas, registers, and so on, condition x and condition y would always yield a different distribution of A and B under hypothesis (\ref{eq:001}).
The RA could thus still hold even if the strong (non-standard) form occurred predominantly in a specific register or mode, and if (additionally) the corpus did not contain very many texts from this register or mode.%
\footnote{In the given example, this might be the case with the DeReKo.
It contains predominantly edited newspaper texts, and the strong forms -- being non-standard -- are probably very rare in such texts.}
However, effects might be more difficult to detect in such a situation; see Section~\ref{sec:nevereverrandomfallacy} for more on this.
In the worst case, a corpus might simply contain not enough exemplars of a certain phenomenon as a result of an inappropriate register or mode composition.
This would be detrimental for any study, but it is also definitely not related to the RA.
In any case, problems with the RA are not exclusive to linguistics or corpus linguistics, but represent standard problems with data sampling for experiments.%
\footnote{``Drawing a random sample of the U.\,S.\ population, in this technical sense, would cost several billion dollars (since it requires a census as a preliminary matter) and would probably require the suspension of major constitutional guarantees. Random sampling is not an idea to be lightly invoked.'' \citep[23]{BerkFreedman2009}}

On the basis of these elaborations, I propose that the core problem for corpus studies in cognitively oriented linguistics is not the RA.
Rather, it is one of the following problems (depending on the research question):

\vspace{\baselineskip}

\begin{itemize}
  \item\label{it:pooled} \textbf{Problem 1}: The corpus contains pooled output data from numerous individual speakers, making it impossible or difficult to draw conclusions about cognitive representations (see \citealt{Dabrowska2008,Dabrowska2012,Dabrowska2015}).
  \item\label{it:meta} \textbf{Problem 2}: The corpus does not contain the relevant meta-information to draw a sample which represents the population of interest in a given study (for example, if genre or register effects are of interest and the corpus does not contain the relevant metadata).
  \item\label{it:composition} \textbf{Problem 3}: Even if the composition of the corpus in terms of registers, modes, etc.\ is not a primary research interest, a phenomenon which occurs only in specific registers, modes, etc.\ might be underrepresented in a given corpus because of its composition.
\end{itemize}

\vspace{\baselineskip}

\textbf{Problem~1} has multiple remedies.
At first sight, it might seem a valid option to enrich corpora with metadata such that the writer\slash speaker of each exemplar can be identified, then taking per-speaker preferences into account (as promoted by \citealt{Gries2015,Gries2017a}).
However, if the mental grammar of each speaker varies (as suggested by \citealt{Dabrowska2008,Dabrowska2012,Dabrowska2015}), then it would not be sufficient to take per-speaker \textit{tendencies} into account in the sense of: \textit{speaker i favours variant A over variant B with probability $p_i$}.
Instead, a separate (statistical) model (or a complex model with per-speaker random slopes) would have to be built for each speaker, since the influencing factors guiding each speaker's grammatical choices would be weighted differently.
This would lead either to extremely complex over-parametrised models (if random intercepts and slopes are used; see Section~\ref{sec:statistics}) or to a lot of different models with no way to come up with an interesting generalisation.
\citet{KupermanBresnan2012} suggest (in the context of an experimental setting) to use multi-model averaging \citep{BurnhamAnderson2002} to take into account the variation between speaker-grammars (see also \citealt{BarthKapatsinski2014}).
While this might be applicable for psycholinguistic experiments, it is clearly not feasible in corpus studies with large numbers of speakers, especially since the models over which one averages should be known theoretical options.
In any case, the per-speaker data would be too sparse in any conceivable corpus to make such over-parametrised modelling feasible.

Another more realistic option is to focus on the cognitive principles that theories of cognition predict should govern the formation of mentally represented grammars.
Such cognitive principles should be observable with considerable stability across groups of speakers.
In \ROWeakN, for example, I used predictions derived from \citet{Koepcke1995} about the prototype representations of weak nouns (in the sense of inter-individual cognitive principles) to derive predictions for the outcome of the corpus study.
In \ROMeasure, I used arguments from grammaticalisation research \citep{Koptjevskaja2001} to argue for plausible general cognitive mechanisms which guide the relevant choice between two pseudo-partitive NP structures.
Based on such assumptions, the use of massively pooled data from large corpora is not unjustified, and it avoids the dangers of data dredging and fishing for spurious correlations \citep{GoodHardin2012}.
Even under the assumption of such theoretical predictions about general cognitive mechanisms, it should become standard practice to evaluate corpus-derived models using experimental techniques to check whether corpus data and reactions by native speakers under controlled conditions converge.
A review of the state of the art was provided by \cite{NewmanSorensenduncan2015}, who enumerate a number of studies showing how corpus data and experimental data converge (such as \citealt{BresnanEa2007,DurrantDoherty2010,GriesWulff2005,GriesEa2005}) and a number of studies where the two types of data led to diverging or only partially converging results (such as \citealt{ArppeJaervikivi2007,Dabrowska2014,Mollin2009}).
When researchers do not achieve convergence, they often try to explain this by differentiating between the actual cognitive construct and whatever the pooled usage data as found in corpora represent.
For example, \citet[411]{Dabrowska2014} lists a number of possible reasons to explain why subjects in her experiment diverged in their word association preferences from collocation measures extracted from corpora.
Alternatively, researchers argue for a more adequate statistical analysis to increase the fit between corpus data and experimental data.
See, for example \cite{DivjakEa2016}, who show that generalised additive models (GAMs) are better suited than generalised linear models (GLMs) for correlating reading times and corpus data.
No general consensus and no commonly accepted best-practice approach has emerged so far, which is not surprising given the number of cognitive constructs assumed at diverse levels, the problems of corpus composition, the operationalisations involved in experiments, and the choice of statistical tools.
Also, experimental validation of corpus-based findings has simply not become a general requirement.
As \citet[3--4]{DivjakEa2016a} put it:

\begin{quote}
  There are now a number of published multivariate models that use data[,] extracted from corpora [\ldots] to predict the choice for one morpheme, lexeme or construction over another.
  However, [\ldots] only a small number of these corpus-based studies have been cross-validated [\ldots].
  Of these cross-validated studies, few have directly evaluated the prediction accuracy of a complex, multivariate corpus-based model on humans using authentic corpus sentences [\ldots].
\end{quote}

Therefore, in \ROMeasure, I used experimental validation in two different paradigms and provide possible explanations for the quality of the fit between the corpus data and the experimental data, much in the spirit of \citet{Dabrowska2014}.%
\footnote{See also \citet{SchaeferPankratz2018}, where a different type of experimental validation of corpus-based findings is used.}

Turning to \textbf{problem~2}, there is a relatively simple solution in theory, which might, however, be difficult to implement in practice.
If the corpus lacks the appropriate metadata, one can simply draw an unrestricted sample and manually annotate the relevant registers, styles, etc.\ afterwards.
While this might sound quite laborious, it also ensures that the relevant categories are the ones the researcher has a theory-driven hypothesis about.
Given the many different definitions of registers and similar categories, it is not likely that corpus creators would annotate a corpus with exactly the taxonomy the researcher has in mind.%
\footnote{Also notice that recent approaches to automatic large-scale register identification failed with a classification accuracy in the region of 50\% \citep{BiberEgbert2016}.}
Of all of my studies collected here, only \ROMeasure\ made reference to style (not register) as an influencing factor.
Fortunately, as one of the creators of the DECOW corpora, I could implement the necessary technology to automatically annotate the corpus with proxy variables to style (see \RAMeasure, also \citealt{SchaeferEa2013}).

Finally, \textbf{problem~3} has a relatively simple solution.
Under the unrestricted query approach (which -- I want to stress once again -- was used for all research collected here), data sparsity can be remedied by creating larger and at the same time more varied corpora.
Research on phenomena which are simply rare in general benefit from sheer corpus size.
In all case studies collected here, one of the examined variants had a low frequency, and the studies all benefitted from the fact that the corpus used (DECOW in different versions) was very large.
In \ROWeakN, the proportion of forms of the weak nouns inflected according to the strong paradigm was reported to be between approximately 1\% and 2\%.
In \RODefArt, the forms of the cliticised indefinite article was estimated to account for roughly 2.5\% of all forms of the indefinite article.
Furthermore, in \ROWeil, clauses headed by \textit{obwohl} `although, then again' and \textit{weil} `because' showing verb-second order made up roughly 6\% to 7\% of all clauses headed by those particles.
Only in \ROMeasure\ was the situation less extreme, with the rarer of the two competing measure NP constructions accounting for approximately 22\% of all exemplars in the sample.

Furthermore, DECOW is a web-derived corpus created by an unrestricted crawl of the German-speaking web.
As such, it contains documents written in all sorts of styles, registers, and text types.
These include sources of non-standard written language such as forums, which is not true of the other very large German corpus, the DeReKo.
The fact that such sources are included in the corpus was vital for at least \RODefArt, \ROWeakN,  and \ROWeil\ because the relevant alternation is only truly productive in non-standard language, as normative grammars ban one of the alternants.%
\footnote{Some normative grammars take similarly clear stances on the phenomenon discussed in \ROMeasure, as pointed out in the paper.
However, the normatively dispreferred variant is still used quite often, despite such attempts to suppress it.}
With any other available corpus, the case studies would have run into problem~3 (data sparsity due to an inappropriate composition of the corpus in terms of registers, modes, text types, etc.).
In Section~\ref{sec:webcorpora}, I therefore provide a short motivation of why web corpora are an important new source of data.

\subsection{Web corpora}
\label{sec:webcorpora}

Web corpora were made popular through the WaCky initiative \citep{BaroniEa2009} starting around 2005.
The WaCky corpora were attractive to many researchers because they were made available freely and could be downloaded fully, which allows for all kinds of local processing not possible through web-based query interfaces.
In parallel, the SketchEngine corpora were developed as a commercial alternative \citep{KilgarriffEa2014}, and the SketchEngine project probably represents the most significant current provider of web-derived linguistic corpora.
The COW corpora have been under development since 2012 \citep{SchaeferBildhauer2012,SchaeferEa2013,SchaeferBildhauer2013,Schaefer2015b,BildhauerSchaefer2016,Schaefer2016a,Schaefer2017,BildhauerSchaefer2017}.
Like the WaCky corpora, they are freely available both for download and via a web interface for easy querying.%
\footnote{\url{https://www.webcorpora.org/}}

While smaller specialised corpora are sometimes derived from web data (\eg\ \citealt{Krause2016}), the major advantages of web data in the context of the research presented here (see especially problem~3 from Section~\ref{sec:problemswithcorporaandsomesolutions}) is that there is a virtually unlimited supply of textual data available on the web.
Also, web data includes non-standard written forms, and the breadth of the variation contained within it is enormous.
For example, in \citet{BildhauerSchaefer2016,BildhauerSchaefer2017}, it was shown that the DECOW16 web corpus has a much broader spread of topics than the DeReKo newspaper corpus.%
\footnote{Additionally, a large-scale analysis of the distribution of automatically extracted lexico-grammatical features in web documents and the DeReKo corpus is being prepared for publication by the Institut für Deutsche Sprache and the present author.}

Thus, web corpora were the obvious choice for the case studies, and the DECOW corpus was created by me specifically for the purpose of conducting my linguistic research published between 2014 and today.%
\footnote{This is not the place to discuss technical details of web corpus construction.
  \citet{SchaeferBildhauer2013} provides a convenient introduction to the subject.}
Given the data from the survey to be published in \citet{Schaefer2019} (see p.~\pageref{abs:survey}), we can assess the impact web corpora have had on current research in corpus linguistics by looking at the frequency with which different corpora have been used in research published in the three major international corpus linguistics journals between 2010 and 2015.
Figure~\ref{fig:corpususe} plots the distribution of the corpora used (328 usages of corpora in 198 papers).

\begin{figure}[htpb]
  \centering
  \includegraphics[width=\textwidth]{graphics/corpususe}
  \caption{Corpora used in the three major corpus linguistics journals; CUSTOM was assigned when authors reported the creation of a custom corpus for the concrete study (a selection of newspaper articles, of academic papers, etc.); LITERATURE was assigned to papers about specific literary works; 111 corpora which were only used once (including DeReKo) are not shown}
  \label{fig:corpususe}
\end{figure}

Figure~\ref{fig:corpususe} clearly shows that the distribution of the use of corpora follows almost a power law distribution.
Custom corpora built for a specific research project are most frequent (used 68 times), followed by the BNC (used 36 times) and a distant third, the COCA (nine times).%
\footnote{I do not even begin to discuss the problems of reproducibility involved when a custom corpus is created ad-hoc for a single research paper.
The problem is even graver when web data is used in an ad-hoc fashion for corpus creation or even gathered by googling \citep{Kilgarriff2006}.}
The English UKWAC web corpus was used only four times, and the French FRWAC and the German DEWAC were each used once.
Given the advantages of web corpora as argued for above, this is a baffling result.
Before turning to an in-depth view of statistical methods used in my research, I therefore wish to point out that web corpora clearly seem to be underused in contemporary corpus linguistics.
I hope the case studies presented below demonstrate their usefulness and inspire other corpus linguists to use them in their research.


