% !Rnw root = ../../main.Rnw
<<setupmsrnsexpr, cache=FALSE, include=FALSE, results='asis'>>=
opts_knit$set(self.contained=FALSE)
@

\section{Experiments}
\label{sec:experimental}

\subsection{Experiment 1: Forced-choice}
\label{sec:exp:fc}

In this section, the probabilities of the alternants (as calculated from attested material in the corpus study) are correlated with the reactions of participants in controlled experiments.
Thus, a direct link is established between output material found in corpora and the behaviour of linguistic agents.

The decision to use attested material and perform a global validation of the corpus-based model was not arbitrary.
Alternatively, one could have created stimuli where the features favouring the alternatives were permuted and thus tested directly.
The answer is related to the statement in Section~\ref{sec:cogocl} about the rich multi-factorial data sets used in corpus studies and the comparatively restricted ones in experiments.
Looking at Table~\ref{tab:variables}, we see one binary factor (\textit{Cardinal}), two three-level factors (\textit{Measurecase} and \textit{Kindgender}) and one five-level factor (\textit{Measureclass}).
Since the dependent variable is binary (\textit{Construction}), permuting these factors alone leads to 180 different possible permutations.
Additionally, the numeric variables \textit{Kindattraction} and \textit{Measureattraction} would also have to be tested at several values.
It is obvious that this is impossible in a controlled experiment, given that a participant can be exposed to no more than roughly one hundred sentences, most of which would be fillers rather than target sentences.
Thus, while the present approach only allows for a global test of the corpus-derived model, this appears to me to be the only feasible way.%
\footnote{For the same reasons, the experimental data are also much too sparse to perform post-hoc analyses with respect to the single regressors in a sound way.}
It is, in the sense of Section~\ref{sec:cogocl}, an optimal synthesis of data-rich multifactorial corpus studies and experimental validation, and it is one that has been used at least since the seminal \citet{BresnanEa2007} paper.
As \citet[3--4]{DivjakEa2016a} pointed out, however, the numbers of published studies including validation is still inadequate.

\subsubsection{Setup, stimuli, and participants}

The first experiment tests preferences for constructions explicitly in a forced-choice task.
Participants had to choose between two sentences that differed only in that one contained the \NACa\, and the other one contained the \PGCa.
In the analysis, the probabilities assigned to the stimuli by the corpus-derived model were compared to the frequency with which participants chose the alternants.
The participants were 24 native speakers of German without reading or writing disabilities, aged 19 to 30, and living permanently in Berlin.
They were recruited from introductory linguistics courses at Freie Universität Berlin.
Although the experiment was conducted in the last four weeks of their first semester, participants had no deeper knowledge of linguistics, grammar, or experimental methods.
None of them had ever participated in a forced-choice experiment before.
Participation was voluntary, but participants received credit in partial fulfillment of course requirements.

As stimuli, attested MNPs from the corpus study were used, but the sentences were simplified to avoid influences from contextual nuisance factors as much as possible.
16 MNPs were sampled from the concordance, making sure that any simplifications and normalisations did not affect any of the regressors used in the corpus study.
In the simplified sentences, the case, number, etc. of the MNP remained the same as in the attested sentence, as did the choice of lexical material within the MNP.
Eight sentences contained masculine or neuter kind nouns, and the other eight contained feminine kind nouns.
Furthermore, in each of the masculine\slash neuter and feminine groups, four sentences originally containing the \NACa\ and four sentences originally containing the \PGCa\ were chosen.
Moreover, the sentences were sampled as typical examples of \PGCa\ (high probability assigned by GLMM) and \NACa\ (low probability assigned by GLMM), respectively.%
\footnote{Remember from Section~\ref{sec:corpusstudies} that the model predicts the probability that the \PGCa\ is chosen over the \NACa.}
High and low probabilities were defined (roughly) as the top and bottom 20\% of all probabilities assigned by the GLMM.
Lemmas and feature combinations were made unique within each group whenever possible.
For calculating the model predictions, the document-level variables \textit{Badness} and \textit{Genitives} were set to 0, which is the mean for z-transformed variables.
The design is summarised in Table \ref{tab:experiment1:design}.

\begin{table}
  \centering
  \caption{The four groups of sentences chosen as stimuli; in each group of four sentences, combinations of important factor values were made unique whenever possible}
  \begin{tabular}[h]{lll}
     & Masculine\slash Neuter & Feminine \\
     \midrule
     \textbf{high prob.\ for PGC\Subsf{adj}} & 4 sentences & 4 sentences \\
     \textbf{low prob.\ for PGC\Subsf{adj}} & 4 sentences & 4 sentences \\
  \end{tabular}
  \label{tab:experiment1:design}
\end{table}

The final pairs of stimuli were the sentences containing the attested and preferred alternant (according to the corpus GLMM) and a modified version containing the dispreferred alternant.
They were presented next to each other, and a 20 second time limit for each choice was set.%
\footnote{No participant ever exceeded the time limit.}
The order of sentences was randomised for each participant, and the position of the alternants on the screen (left\slash right) was randomised per participant and sentence.
As fillers, 23 pairs of sentences exemplifying similar but unrelated alternation phenomena from German morphosyntax were used.
Thus, participants saw 39 pairs of sentences and 78 sentences in total.
They were instructed to select from each pair of sentences the one that seemed more natural to them in the sense that they would use it rather than the other one.
The experiment was conducted using \textit{PsychoPy} \citep{Peirce2007}.

\subsubsection{Statistical model}

A multilevel logistic regression was specified with the probability of the \PGCa\ predicted for each sentence by the corpus-based GLMM as the only fixed effect \textit{Modelprediction}.
A random intercept was added for the individual sentence pair (\textit{Item}) in order to catch idiosyncrasies of single sentences.
Coefficients were estimated with Maximum Likelihood Estimation (\textit{lmer} function from \textit{lme4}).
The number of observations was \textit{n=}384.

A good amount of the variance can be accounted for by idiosyncrasies of single sentences ($\sigma_{\text{Item}}=1.217$).
Also, among participants, there are clearly different preferences ($\sigma_{\text{Participant}}=0.412$).
On the extreme ends, one participant chose the \PGCa\ in 13 of 16 cases, and two participants only chose it in 5 of 16 cases.
The regressor \textit{Modelprediction} achieves $\mpPB=0.003$ (1,000 replications) and is estimated at 4.389 relative to an intercept of -1.270.
The confidence interval from a parametric bootstrap (1,000 replications, percentile method) for the regressor is acceptable but a tad large with a lower bound of 1.788 and an upper bound of 6.599.
The pseudo-coefficients of determination are $R^2_{m}=0.185$ and $R^2_{c}=0.455$, which means that roughly 19\% of the variance in the data can be explained by considering only the predictions from the corpus-based GLMM.

\subsubsection{Interpretation}

\begin{figure}[htbp!]
\centering
%\includegraphics[width=0.5\textwidth]{../R/output/fc_effects}
\caption{Effect plot for the multilevel logistic regression in the forced-choice experiment: predictability of participants' choices using the probabilities derived from the corpus-based GLMM}
\label{fig:afc:effects}
\end{figure}

The marginal $R^2$ indicates a weak result for the fixed effects part of the model, which is nonetheless worthy of mention (close to 0.2).
The effect display for the single fixed regressor \textit{Modelprediction} is given in Figure \ref{fig:afc:effects}.
The higher the probability of the \PGCa\ predicted from usage data, the more often participants chose the \PGCa\ alternant in the forced-choice task.
A closer look at the results in the form of the spine plot in Figure~\ref{fig:spines} shows, however, that it was likely an idiosyncracy of a single sentence which spoiled an otherwise much better correlation.
The problematic sentence with a model prediction of 0.548 is given in (\ref{ex:problem}) in the \PGCa\ variant.

\begin{exe}
  \ex{\label{ex:problem}\gll Man machte mal wieder viel Lärm um [jede Menge [heißer Luft]\Sub{Gen}]\Sub{Acc}.\\
  one made once again much noise about any amount hot air\\
  \trans People made much ado about nothing once again.}
\end{exe}

\noindent In retrospect, this stimulus was badly chosen because \textit{heiße Luft} `ado' (literally `hot air') is a fixed metaphorical expression.
This obviously influences the reactions of participants, and a revised and improved experiment might lead to a much better fit in future research.%
\footnote{Since I am convinced that the statistical analysis of an experiment has to be determined as part of the experiment's design before the experiment is conducted, removing the data points corresponding to the problematic stimulus from the analysis is not an option.
Future research with improved stimuli is the proper route to take.}

\begin{figure}[htbp!]
  \centering
  %\includegraphics[width=0.5\textwidth]{../R/output/fc_proportions}
  \caption{Spine plot of the proportion of responses plotted against the predictions from the corpus-based model in the forced-choice experiment}
  \label{fig:spines}
\end{figure}

In principle, random slope for \textit{Modelprediction} varying by item could also remedy problems with individual stimuli at least partially.
Therefore, a model with random slopes for \textit{Modelprediction} varying by both random effects (\textit{Participant} and \textit{Item}) was specified and estimated.
The random slope for participants was added to comply with \citet[257]{BarrEa2013} who predict ``catastrophically high Type I error rates'' for experimental designs with within-subject manipulations if random effects structures are not kept maximal.
The coefficient of the fixed effect changed noticeably but not enough to change the interpretation (5.408 relative to an intercept of -1.304), and the marginal $R^2_m$ rises to 0.213 ($R^2_c=0.488$).
In line with expectations, the standard deviation in the random slopes for \textit{Item} is high at 5.996.
However, the covariance parameters were estimated at exactly -1, which is a clear sign that the variance-covariance matrix could not be estimated successfully.
The same was true for models with only an \textit{Item} or only a \textit{Participant} random slope.
This is exactly the kind of model overparametrisation criticised in \citet{BatesEa2015a} and \citet{MatuschekEa2017}.
The available data are insufficient to estimate the parameters of the more complex model with varying slopes.%
\footnote{\citet[1]{BatesEa2015a} state: ``We show that failure to converge typically is not due to a suboptimal estimation algorithm, but is a consequence of attempting to fit a model that is too complex to be properly supported by the data, irrespective of whether estimation is based on maximum likelihood or on Bayesian hierarchical modelling with uninformative or weakly informative priors.
Importantly, even under convergence, over-parameterisation may lead to uninterpretable models.''
}

In summary, the forced-choice experiment succeeded in corroborating the results from the corpus study inasmuch as the preferences extracted from usage data correspond to native speakers' choices, but the correlation is weak, likely at least in part due to problems with individual test items and\slash or too sparse data.


\subsection{Experiment 2: Self-paced reading}
\label{sec:exp:spr}

\subsubsection{Setup, stimuli, and participants}

The second experiment was conducted in a more implicit paradigm.
It is expected that reading less typical alternants in a given context and with given lexical material incurs a processing overhead for the reader (\citealp{Kaiser2013}).
In this section, a self-paced reading experiment is presented.
In a very similar fashion, \citet{DivjakEa2016} apply the self-paced reading paradigm in the validation of corpus-based models.
The analysis compares the corpus-derived probabilities with potential lags in reading time for sentences with the preferred and the non-preferred constructions.

The exact same stimuli as in the forced-choice experiments were used.
Each participant read both the 16 sentences with the alternant predicted by the corpus model and the 16 modified sentences with the alternant that the corpus model did not predict.%
\footnote{Notice that lemmas and their frequencies as well as lemma classes are included as regressors in the corpus-based GLMM, and there was consequently no additional controlling of lemma frequencies, etc.}
To minimise repetition effects, the stimuli for each participant were separated into two blocks of 16 targets and 33 fillers per block.
In the experiment, participants first read all sentences from the first block, then all sentences from the second block.
From each target sentence pair, one sentence was assigned to the first block and the other sentence to the other block.
The assignment of members of the individual sentence pairs to the blocks was randomised for each participant individually, as was the order within each block.
The fillers also came in pairs such that the second block exclusively contained sentences to which participants had been exposed in the first block in slightly modified form.
In total, each participant read 98 sentences.
After each sentence, participants had to answer simple (non-metalinguistic) yes-no questions about the previous sentence as distractors.
The distractor questions were different between the first and the second blocks.
There were 38 participants recruited in exactly the same manner as for the experiment reported in Section~\ref{sec:exp:fc}.
None of them had ever participated in any kind of reading experiment, and none of them took part in the first experiment.
The experiment was conducted using \textit{PsychoPy}.

The reading times were residualised per speaker based on the reading times of all words (not just the targets) for that speaker.
The adjective and the kind noun (\ie\ the constituents bearing the critical case markers) were used as the target region, for instance the bracketed words in the example \textit{zwei Gläser} [\textit{spru\-deln\-des Wasser}] `two glasses of sparkling water'.
Outliers farther than 2 inter-quartile ranges from the mean logarithmised residualised reading time were removed (64 data points), resulting in a total number of \textit{n=}1,152 observations.

\subsubsection{Statistical model}

An LMM was specified with the logarithmised residual reading times as the response variable.
The probabilities derived from the corpus GLMM (\textit{Modelprediction}) were added as the main regressor of interest.
It should be remembered that the higher the GLMM prediction is, the more typical the sentence is for containing the \PGCa.
It is therefore expected that reading times are higher when the value of \textit{Modelprediction} is higher but the sentence contains the \NACa.
However, when the sentence contains the \PGCa, reading times should be lower when \textit{Modelprediction} is higher.
To account for this, an interaction between \textit{Modelprediction} and \textit{Construction} (levels \textit{PGCadj} and \textit{NACadj}) was added to the model.

Furthermore, the position (1--98) of the sentence in the individual experiment (\textit{Position}) was included as a fixed effect to control for the usual increase in reading speed during an experiment.
Random intercepts were specified for \textit{Participant} and \textit{Item} (the 16 sentence pairs are one \textit{Item} each).%
\footnote{Again, all attempts to include random slopes resulted in the variance-covariance matrix not being properly estimated (1 or -1 covariance parameters).}

\begin{table}
  \centering
  \caption{Fixed effect coefficient table for the LMM used to analyse the self-paced reading experiment; the intercept is 0.829}
  \begin{tabular}{lrrrc}
    Regressor & \multicolumn{1}{r}{Coefficient} & \multicolumn{1}{r}{CI low} & \multicolumn{1}{r}{CI high} & \multicolumn{1}{r}{CI excludes 0} \\ \midrule
    Construction=PGCadj                 &  0.054 &  0.012 &  0.095 &  *  \\
    Modelprediction                     & -0.003 & -0.113 &  0.110 &     \\
    Position                            & -0.005 & -0.005 &  0.004 &     \\
    Construction=PGCadj:Modelprediction & -0.125 & -0.234 & -0.023 &  *  \\
  \end{tabular}
  \label{tab:exp:spr}
\end{table}

Table~\ref{tab:exp:spr} shows the coefficient estimates with a 95\% parametric bootstrap confidence interval (1,000 replications, percentile method).
The standard deviation of the participant intercepts is $\sigma_{\text{Participant}}=0.079$ and of the item intercepts $\sigma_{\text{Item}}=0.037$.
Comparing the full model to a model without the main regressor \textit{Modelprediction} (and consequently also without the interaction with \textit{Construction}) in a PBmodcomp test gives $\mpPB=0.036$.
The pseudo-coefficients of determination are $R^2_m=0.239$ and $R^2_c=0.346$.

An alternative Gaussian generalised additive model with an identity link was also fit (see \citealp{DivjakEa2016}) using the \textit{mgcv} package \citep{Wood2011}.
The full results are included in the data package for this paper, but the fit was not better than with the LMM reported above.
The estimated smoother for the \textit{Modelprediction} variable is essentially linear, and the $R^2$ (corresponding to the marginal $R^2$ of the LMM) was 0.237.

\subsubsection{Interpretation}

The coefficients of determination indicate that there is a noteworthy correlation between the reactions of the subjects and the corpus-derived probabilities (marginal $R^2$) and that there is some between-subject variation: the difference between $R^2_m$ and $R^2_c$ is 0.107.

\begin{figure}[htbp!]
\centering
%\includegraphics[width=0.5\textwidth]{../R/output/spr_effects}
\caption{Effect plot for the LMM in the self-paced reading experiment: modelling participants' residualised log reading times on the probabilities given by the corpus-based GLMM}
\label{fig:spr:effects}
\end{figure}

The effect plot for the interaction of interest is shown in Figure~\ref{fig:spr:effects}.
The estimate for the sentences with \NACa\ is obviously imprecise, and no significant differences in reading times are observed.
There is a clearer effect in the sentences with \PGCa, which is also confirmed by the significant results from the bootstrapped confidence intervals (see Table~\ref{tab:exp:spr}) and from the PBmodcomp test reported above.
The \PGCa\ brings about an increased reading time, which is plausible because it is the much rarer construction (see Section~\ref{sec:corpusstudies}).
However, if it occurs in a prototypical context and with typical lexical material, reading times drop.
This can be seen in the downward slope in the right panel of Figure~\ref{fig:spr:effects}.
This fits into the general picture inasmuch as the construction with the lower frequency might be developing towards a more sharply defined prototype.%
\footnote{In this context, it should be remembered from Section~\ref{sec:corpusstudies} that even the \PGCd\ is much rarer that the \NACb\ (17,252 vs.\ 315,635 occurrences in the auxiliary corpus samples).}
Conversely, the \NACa\ (like the NAC in general) might be the highly frequent default which does not incur a reading time penalty, even if it is not the optimal choice in the given context and with the given lexical material.

In Section~\ref{sec:conclusion}, I take stock and summarise the contributions of the present study to the research on alternations in cognitive linguistics.
