\section{Prototypes, exemplars, and corpora in cognitive linguistics}
\label{sec:cogocl}

\subsection{Alternations}
\label{sec:alternations}

This paper deals with a morphosyntactic alternation between two measure noun constructions in German.
By \textit{alternation} I refer to a situation where two or more forms or constructions are available with no clear difference in acceptability, function, or meaning.
The study of lexical and constructional alternations has a long history in cognitive corpus linguistics (for example, \citealp{BresnanEa2007}; \citealt{BresnanHay2010}; \citealt{BresnanFord2010}; \citealt{DivjakArppe2013}; \citealt{Gries2015}; \citealt{NessetJanda2010}).
This research is based on the assumption that language is a probabilistic phenomenon where alternants are chosen neither deterministically nor fully at random \citep{Bresnan2007}.
Multifactorial models are constructed which incorporate influencing factors from diverse levels, including lexical and contextual factors.
The estimation of the model coefficients quantifies the influence which the factors have on the probability that either alternant is chosen.
There are two fundamental issues to consider with respect to this tradition.
First, there is the question of whether corpus data provide any insight into cognitive representations at all.
This question can and should be answered by testing how well corpus-derived models converge with or diverge from experimental findings (Section~\ref{sec:usagedataincognitivelinguistics}).
Second, the appropriate modelling of such results in cognitive linguistics is a key issue (Section~\ref{sec:prototypesexemplarscorporaandcontrolledexperiments}).

\subsection{Usage data in cognitive linguistics}
\label{sec:usagedataincognitivelinguistics}

For some time, there has been an interest in correlating probabilistic generalisations extracted from corpus data and results from experimental work \citep{ArppeJaervikivi2007,BresnanEa2007,BresnanFord2010,DivjakGries2008,DivjakEa2016,FordBresnan2013}.
This is often called a \textit{validation} of the corpus-derived findings, but \citet[303]{Divjak2016a} criticises this choice of words ``because it creates the impression that behavioral experimental data is inherently more valuable than textual data'', citing \citet{TummersEa2005}, who state that a corpus is ``a sample of spontaneous language use that is (generally) realized by native speakers''.
See also \citet{Newman2011} for a positive view of corpora as a source of data in cognitive linguistics in their own right.
However, as \citet[486--487]{Dabrowska2016} argues, this does not mean that we can in some way ``deduce mental representations from patterns of use'', \ie\ from corpus data.
It would indeed be highly surprising if this were possible, and the same holds for experimental methods.
Nobody assumes that we can inductively infer mental representations from experiments, which even allow for direct access to the cognitive agent and offer much better ways of controlling experimental conditions and nuisance variables.
Rather, predictions are derived from existing theories in order to test these theories.
While this approach is applicable to corpus data, I discuss some relevant differences below.

As mentioned above, the central question is whether usage data as found in corpora are truly predictive of speakers' and writers' cognitive representation of language and\slash or of their linguistic behaviour.
This is where the experimental validation (or corroboration) comes into play.
A review of the state of the art was provided by \citet{NewmanSorensenduncan2015}, who enumerate a number of studies showing how corpus data and experimental data converge (such as \citealp{BresnanEa2007,DurrantDoherty2010,GriesWulff2005,GriesEa2005}) and a number of studies where the two types of data led to diverging or only partially converging results (such as \citealp{ArppeJaervikivi2007,Dabrowska2014,Mollin2009}).
When researchers do not achieve convergence, they often try to explain this by differentiating between the actual cognitive construct and whatever the pooled usage data as found in corpora represent.
For example, \citet[411]{Dabrowska2014} lists a number of possible reasons to explain why subjects in her experiment diverged in their word association preferences from collocation measures extracted from corpora.
Alternatively, researchers argue for a more adequate statistical analysis to increase the fit between corpus data and experimental data.
See, for example \citet{DivjakEa2016}, who show that generalised additive models (GAMs) are better suited than generalised linear models (GLMs) for correlating reading times and corpus data. 
Much more optimistically, \citet{StefanowitschFlach2016} proposed a straightforward positivist perspective of corpora as representing the input of an average adult speaker, thus licensing inferences from corpus data to cognitive representations under a ``corpus-as-input'' view.
They state that ``in this wider context, large, register-mixed corpora such as the British National Corpus [\ldots] may not be perfect models of the linguistic experience of adult speakers, but they are reasonably close to the input of an idealized average member of the relevant speech community'' \citep[104]{StefanowitschFlach2016}.
If this were a valid assumption then any generalisation extracted from the BNC could be assumed to have some kind of mental reality, which is doubtful.

No general consensus has emerged yet, which is not surprising given the number of cognitive constructs assumed at diverse levels, the problems of corpus composition, the operationalisations involved in experiments, and the choice of statistical tools.
While far from providing definitive solutions, my discussion in Section~\ref{sec:conclusion} will provide possible explanations for the quality of the fit between the corpus data and the experimental data reported in Sections~\ref{sec:corpusstudies} and~\ref{sec:experimental}, much in the spirit of \citet{Dabrowska2014}.

\subsection{Prototypes, exemplars, corpora, and controlled experiments}
\label{sec:prototypesexemplarscorporaandcontrolledexperiments}

I now turn to cognitive representations as constructs in alternation research.
The typical approach in alternation research is to annotate a large number of corpus sentences with linguistic features and to model the probability of the variants being chosen given these features.
The idea is that a variant is chosen when the influencing features have certain typical values.
In other words, a variant of prototype theory with features \citep{Rosch1978} is the appropriate model, also because the features used to feed the model are usually abstract high-level linguistic features. 
Some researchers such as \citet{Gries2003}, \citet{NessetJanda2010}, or \citet{Schaefer2016c} indeed commit to prototype theory in alternation modelling.
Under prototype theory, category membership is defined by similarity to an ideal exemplar and its characterising features (see \citealp{Taylor2008} for an overview).

While prototype theory is well suited for modelling constructional choices, it is just one of at least two major similarity-based theories of classification, the prominent alternative being exemplar theory (\citealp{MedinSchaffer1978,Hintzman1986}).
Prototype theory and exemplar theory model essentially the same effects when only output data are considered.
The theories differ significantly in whether they assume higher-level abstractions in the form of single maximally prototypical exemplars (prototype theory) or categories emerging through the storage of many exemplars and similarity-based classification over those sets of exemplars (exemplar theory).
\citet{Barsalou1990} already showed that prototype and exemplar theory model the same types of surface effects and are informationally equivalent.
Consequently, experiments which favour one theory over the other use procedural behaviour of subjects in experiments, for example the speed of category retrieval, as opposed to mere output data.%
\footnote{See \citet{StormsEa2000} for a comparison of the theories in different experimental settings.}
Since corpus data only show artefacts of production events and we have no experimental access to the speaker's or writer's performance and their actual similarity judgements, one should be sceptical whether corpus analysis alone could ever help to decide which theory of mental representation is more suitable (see also \citealp[22]{Gries2003} and the quote from \citealp[486--487]{Dabrowska2016} as cited above).
However, as I will show, some effects are naturally analysed as prototype effects while others are almost necessarily exemplar effects.

In cognitive science, it is mostly accepted that exemplar theories have greater explanatory power \citep[184]{Vanpaemel2016}, and that abstraction is only needed marginally, if at all.
Still, various attempts have been made to settle the dispute between abstraction-based models (models with rules or prototypes) and exemplar models.
\citet{VanpaemelStorms2008} and \citet{LeeVanpaemel2008} proposed the \textit{varying abstraction model} (VAM) which ``attempt[s] to balance economy and informativeness'' \citep[745]{LeeVanpaemel2008}, treating models with full abstraction (radical prototype theory) and no abstraction at all (radical exemplar theory) as special cases of a flexible approach which allows both types of mechanisms.
The mixture model of categorisation (MMC) by \citet{Rosseel2002} is a model with abstraction in the form of hierarchical clusters of exemplars.
These clusters of objects are characterised by a probability distribution over their features, and categorising new objects is a process of estimating the probability of this object belonging to one of the clusters.
\citet{GriffithsEa2009} go further and present a computational model which is able to choose the appropriate complexity of representation for a given category.
However, despite these and more attempts to reconcile or unite the two approaches \citet[183--184]{Vanpaemel2016} describes the state of affairs between adherents of neo-prototype theory (such as \citealp{MindaSmith2001,MindaSmith2002}) and exemplar theory as a stalemate.


In cognitive linguistics, similarity-based categorisation is often seen as the central and conceptually sufficient approach to linguistic categorisation.
See \citet{Taylor2003} for a comprehensive treatment in terms of prototype theory and \citet{Taylor2012} for a detailed discussion of an exemplar-based approach.
\citet{DivjakArppe2013} is a rare example of a paper in cognitive linguistics where a synthesis between prototype and exemplar models is proposed.
Their corpus-based approach shows ``one way of systematically analyzing usage data as contained in corpora to yield a scheme, compatible with usage-based theories of language, by which the assumptions of both the prototype and exemplar theories can be operationalized'' \citep[267]{DivjakArppe2013}.
Their approach to implementing a varying abstraction model \citep[254--260]{DivjakArppe2013} is based on hierarchical clustering of annotated properties of sentences.
They cluster sentences containing Russian verbs of trying.
Then, they single out the one sentence from each cluster which scores the highest probability for any of the six \textit{try} verbs according to a polytomous regression model estimated on the same data.
The clusters are interpreted as intermediate-level exemplar-derived abstractions of typical contexts for these high-probability verbs (typically more than one cluster for each verb; \citealp[255--256]{DivjakArppe2013}).
The crucial difference between such data-driven corpus-based analyses and experiments in cognitive science (\citealp{DivjakArppe2013} use \citealp{VerbeemenEa2007} as their reference) is that cognitive research is based on experiments where subjects produce actual category assignments or similarity judgements, and in corpus studies, the categories and category membership are determined purely from existing data.
The experimental approach with reduced and\slash or artificial stimuli makes it much easier to examine very specific effects in the behaviour of the subjects.
While I am convinced that the results presented in \citet{DivjakArppe2013} are valid and important (especially given the previous and subsequent research the authors have conducted on the data, including experimental work presented in \citealp{DivjakGries2008,DivjakEa2016}), any data set can be clustered to yield a certain number of clusters.
Thus, the corpus study alone does not ensure that the clusters emerging from the data correspond to any particular speaker's cognitive representation.

On the other hand, the trade-off one has to accept when doing experiments with highly simplified stimuli is lower \textit{external validity} (\ie\ decreased generalisability) and higher dependence on proper operationalisations of constructs (\textit{construct validity}).%
\footnote{An accessible overview of the different types of validity can be found in Chapter~1 of \citet{MaxwellDelaney2004}.}
Tasks in cognitive science have been criticised exactly for their lack of external validity, for example by \citet{Murphy2003}.
From a linguistic perspective, it seems remarkable that \citet[1013]{VoorspoelsEa2011} consider their experimental task -- the assignment of typicality scores to nouns from the domains of \textit{animals} and \textit{artefacts} to categories such as \textit{bird}, \textit{fish}, \textit{clothing}, or \textit{tools} -- a study of ``superordinate natural language categories, whereas most evidence supporting exemplar representations has been found in artificial categories of a more subordinate level''.
Corpus linguists interested in probabilistic alternation modelling often investigate significantly more complex high-level categories and use large and complex feature sets.%
\footnote{However, approaches have emerged which model linguistic phenomena without reference to high-level linguistic features altogether \citep{BaayenEa2016,RamscarPort2016}.}
It is thus an advantage of much linguistic work on categorisation that it deals with complex and realistically produced data because this greatly improves the external validity of studies, although by potentially sacrificing some construct validity.
Thus, the ideal contribution to the research on category abstraction by cognitive corpus linguists is to provide analyses which have high external validity and complexity while carefully making sure that these finding correlate with actions and reactions under more controlled experimental conditions, thus increasing the construct validity.

This paper contributes to solving the questions raised in this section in many ways.
After a thorough description of the alternation under examination, I discuss potential influencing factors comprising high-level abstract semantic generalisations as well as exemplar-similarity and item-specific effects in Section~\ref{sec:germanmeasurenps}.
I will argue that there are lemma-specific and exemplar effects but also generalisations at the level of the construction as well as generalisations overlaying the lemma-specific effects, leading to a complex hierarchical structure.
In Section~\ref{sec:corpusstudies}, I present a corpus study and report a true multilevel generalised linear model with the appropriate hierarchical structure for the hypothesised effects.
In Section~\ref{sec:experimental}, I test the predictions of the corpus-based model in two experimental paradigms (forced-choice and self-paced reading), demonstrating that they indeed converge, albeit with low effect strength.
In Section~\ref{sec:conclusion}, I interpret the findings in the light of the issues of cognitive representations and corpus data and the convergence of usage data and experimental data.

